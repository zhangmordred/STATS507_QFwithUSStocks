{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "import re\n",
    "from six import BytesIO\n",
    "import matplotlib.dates as mdates\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import pymysql.cursors\n",
    "import spacy\n",
    "import ftfy\n",
    "import ast\n",
    "import math\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_date(date_str, days_before):\n",
    "    date_obj = datetime.strptime(date_str, '%Y/%m/%d')\n",
    "    before_date = date_obj - timedelta(days=days_before)\n",
    "    before_date_str = before_date.strftime('%Y/%#m/%#d')\n",
    "    return before_date_str\n",
    "\n",
    "def get_above_date(date_str, days_after):\n",
    "    date_obj = datetime.strptime(date_str, '%Y/%m/%d')\n",
    "    after_date = date_obj + timedelta(days=days_after)\n",
    "    after_date_str = after_date.strftime('%Y/%#m/%#d')\n",
    "    return after_date_str\n",
    "\n",
    "get_above_date('2015/12/30', 3)\n",
    "get_previous_date('2015/12/10', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from the Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily data for stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from each parquet file\n",
    "\n",
    "#file1\n",
    "df = pd.read_parquet(\"train-00000-of-00004.parquet\")\n",
    "df.to_csv(\"stockdata1.csv\", index=False)\n",
    "us_1 = df['symbol'].unique()\n",
    "\n",
    "#file2\n",
    "df2 = pd.read_parquet(\"train-00001-of-00004.parquet\")\n",
    "df2.to_csv(\"stockdata2.csv\", index=False)\n",
    "us_2 = df2['symbol'].unique()\n",
    "\n",
    "#file3\n",
    "df3 = pd.read_parquet(\"train-00002-of-00004.parquet\")\n",
    "df3.to_csv(\"stockdata3.csv\", index=False)\n",
    "us_3 = df3['symbol'].unique()\n",
    "\n",
    "#file4\n",
    "df4 = pd.read_parquet(\"train-00003-of-00004.parquet\")\n",
    "df4.to_csv(\"stockdata4.csv\", index=False)\n",
    "us_4 = df4['symbol'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of stock name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge 4 parquet\n",
    "array1 = np.array(us_1)\n",
    "array2 = np.array(us_2)\n",
    "array3 = np.array(us_3)\n",
    "array4 = np.array(us_4)\n",
    "\n",
    "stock_list = np.concatenate([array1, array2, array3, array4])\n",
    "stock_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance sheet for stocks (to calculate turnover rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_parquet(\"stock_sheet.parquet\")\n",
    "df5.to_csv(\"balance_sheet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the data before 2010 & Only leave symbol, date, close and volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for daily price files\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df1_new = df[(df[\"date\"] >= \"2010-01-01\") & (df['date']<= \"2024-12-31\")][[\"symbol\", \"date\", \"volume\", \"adj_close\"]]\n",
    "\n",
    "df2[\"date\"] = pd.to_datetime(df2[\"date\"])\n",
    "df2_new = df2[(df2[\"date\"] >= \"2010-01-01\") & (df2['date']<= \"2024-12-31\")][[\"symbol\", \"date\", \"volume\", \"adj_close\"]]\n",
    "\n",
    "df3[\"date\"] = pd.to_datetime(df3[\"date\"])\n",
    "df3_new = df3[(df3[\"date\"] >= \"2010-01-01\") & (df3['date']<= \"2024-12-31\")][[\"symbol\", \"date\", \"volume\", \"adj_close\"]]\n",
    "\n",
    "df4[\"date\"] = pd.to_datetime(df4[\"date\"])\n",
    "df4_new = df4[(df4[\"date\"] >= \"2010-01-01\") & (df4['date']<= \"2024-12-31\")][[\"symbol\", \"date\", \"volume\", \"adj_close\"]]\n",
    "\n",
    "#merge all the data\n",
    "df_stockdata = pd.concat([df1_new, df2_new, df3_new, df4_new], ignore_index=True)\n",
    "df_stockdata.to_csv('stocklist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stockdata = pd.read_csv('stocklist.csv')\n",
    "df_stockdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for balance sheet data\n",
    "df5[\"date\"] = pd.to_datetime(df5[\"date\"])\n",
    "dfshares = df5[(df5[\"date\"] >= \"2010-01-01\") & (df5['date']<= \"2024-12-31\")][[\"symbol\", \"date\", \"common_stock_shares_outstanding\"]]\n",
    "\n",
    "#save to csv\n",
    "dfshares.to_csv(\"bs2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave the stocks which are both included in two dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_symbols = set(dfshares[\"symbol\"]).intersection(set(df_stockdata[\"symbol\"]))\n",
    "dfshares = dfshares[dfshares[\"symbol\"].isin(common_symbols)]\n",
    "df_stockdata = df_stockdata[df_stockdata[\"symbol\"].isin(common_symbols)]\n",
    "\n",
    "#check whether the length of the financial data and the trading data are the same\n",
    "len(dfshares['symbol'].unique()) == len(df_stockdata['symbol'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the stocks do not have data before 2023/4 (according to the setting of the experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stockdata[\"date\"] = pd.to_datetime(df_stockdata[\"date\"])\n",
    "\n",
    "# the setting time\n",
    "cutoff_date = pd.Timestamp(\"2023-04-01\")\n",
    "\n",
    "# the earlist trading day for each stock\n",
    "first_trade_dates = df_stockdata.groupby(\"symbol\")[\"date\"].min()\n",
    "\n",
    "#only save stocks had traded before 2023/04\n",
    "valid_symbols = first_trade_dates[first_trade_dates < cutoff_date].index\n",
    "\n",
    "# save stocks in all sets\n",
    "df_stockdata = df_stockdata[df_stockdata[\"symbol\"].isin(valid_symbols)]\n",
    "dfshares = dfshares[dfshares[\"symbol\"].isin(valid_symbols)]\n",
    "len(dfshares['symbol'].unique()) == len(df_stockdata['symbol'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get timeline (align with balance sheet one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeline = df_stockdata['date'].unique()\n",
    "df_timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of the stocks whehter have transaction exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all trading dates\n",
    "all_trading_dates = set(df_timeline)\n",
    "\n",
    "# not satify stocks\n",
    "stocks_with_missing_dates = []\n",
    "\n",
    "for symbol, group in df_stockdata.groupby(\"symbol\"):\n",
    "    stock_dates = set(group[\"date\"])\n",
    "    \n",
    "    #the first trade date of the stock\n",
    "    first_trading_day = min(stock_dates)\n",
    "    last_trading_day = max(stock_dates)\n",
    "    \n",
    "    # the whole tading dates that the stock should have\n",
    "    required_dates = {d for d in all_trading_dates if first_trading_day <= d <= last_trading_day}\n",
    "    \n",
    "    # check whether miss any\n",
    "    if stock_dates != required_dates:\n",
    "        stocks_with_missing_dates.append(symbol)\n",
    "\n",
    "#the abnormal stock\n",
    "print(\"These stocks cannot cover all the trading days：\")\n",
    "print(stocks_with_missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stockdata = df_stockdata[~df_stockdata[\"symbol\"].isin(stocks_with_missing_dates)]\n",
    "dfshares = dfshares[~dfshares[\"symbol\"].isin(stocks_with_missing_dates)]\n",
    "len(dfshares['symbol'].unique()) == len(df_stockdata['symbol'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stockdata['date'] = pd.to_datetime(df_stockdata['date'])\n",
    "\n",
    "# add year-month\n",
    "df_stockdata['year_month'] = df_stockdata['date'].dt.to_period('M')\n",
    "\n",
    "def summarize_month(group):\n",
    "    volume_sum = group['volume'].sum()\n",
    "    first_price = group.iloc[0]['adj_close']\n",
    "    last_price = group.iloc[-1]['adj_close']\n",
    "    adj_close_change = (last_price - first_price) / first_price if first_price != 0 else None\n",
    "    return pd.Series({'volume': volume_sum, 'adj_close_change': adj_close_change})\n",
    "\n",
    "df_stockdata_monthly = df_stockdata.sort_values('date').groupby(['symbol', 'year_month']).apply(summarize_month).reset_index()\n",
    "\n",
    "df_stockdata_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to the csv\n",
    "df_stockdata_monthly.to_csv('stocklist_monthly.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge df_stockdata_monthly and dfshares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge directly, not consider NaN\n",
    "dfshares['date'] = pd.to_datetime(dfshares['date'])\n",
    "dfshares['year_month'] = dfshares['date'].dt.to_period('M').astype(str)\n",
    "df_stockdata_monthly['year_month'] = df_stockdata_monthly['year_month'].astype(str)\n",
    "\n",
    "# create a dict\n",
    "shares_lookup = {\n",
    "    (row.symbol, row.year_month): row.common_stock_shares_outstanding\n",
    "    for row in dfshares.itertuples(index=False)\n",
    "}\n",
    "\n",
    "# add by apply\n",
    "df_stockdata_monthly['common_stock_shares_outstanding'] = df_stockdata_monthly.apply(\n",
    "    lambda row: shares_lookup.get((row.symbol, row.year_month), None),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#fill the table\n",
    "# fill to head\n",
    "df_stockdata_monthly['common_stock_shares_outstanding'] = (\n",
    "    df_stockdata_monthly.groupby('symbol')['common_stock_shares_outstanding']\n",
    "    .ffill()\n",
    ")\n",
    "\n",
    "# fill to tail\n",
    "df_stockdata_monthly['common_stock_shares_outstanding'] = (\n",
    "    df_stockdata_monthly.groupby('symbol')['common_stock_shares_outstanding']\n",
    "    .bfill()\n",
    ")\n",
    "\n",
    "df_stockdata_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the final df_stockdata_monthly\n",
    "df_stockdata_monthly['turnover_rate'] = (\n",
    "    df_stockdata_monthly['volume'] / df_stockdata_monthly['common_stock_shares_outstanding']\n",
    ")\n",
    "\n",
    "df_stockdata_monthly['turnover_rate'] = df_stockdata_monthly['turnover_rate'].round(4)\n",
    "\n",
    "# delete rows where turnover_rate is NaN\n",
    "df_stockdata_monthly = df_stockdata_monthly.dropna(subset=['turnover_rate'])\n",
    "\n",
    "#delete NaN of the shares\n",
    "df_stockdata_monthly = df_stockdata_monthly.dropna(subset=['common_stock_shares_outstanding'])\n",
    "\n",
    "df_stockdata_monthly['year'] = df_stockdata_monthly['year_month'].str[:4].astype(int)\n",
    "\n",
    "# count how many data before 2020 (for training)\n",
    "symbol_pre2020_counts = (\n",
    "    df_stockdata_monthly[df_stockdata_monthly['year'] < 2020]\n",
    "    .groupby('symbol')\n",
    "    .size()\n",
    ")\n",
    "\n",
    "#find the symbol that includes less 20 data \n",
    "symbols_to_filter_out = symbol_pre2020_counts[symbol_pre2020_counts < 20].index.tolist()\n",
    "\n",
    "# delete the data\n",
    "df_stockdata_monthly = df_stockdata_monthly[\n",
    "    ~((df_stockdata_monthly['symbol'].isin(symbols_to_filter_out)) & (df_stockdata_monthly['year'] < 2020))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# delete the \"year\" colume\n",
    "df_stockdata_monthly.drop(columns=['year'], inplace=True)\n",
    "\n",
    "df_stockdata_monthly\n",
    "\n",
    "#save to the csv\n",
    "df_stockdata_monthly.to_csv(\"stocklist_monthly.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_ready = df_stockdata_monthly[['symbol', 'year_month', 'adj_close_change', 'turnover_rate']].copy()\n",
    "\n",
    "# align with the df\n",
    "df_pivot_ready = df_pivot_ready.rename(columns={\n",
    "    'adj_close_change': 'profit',\n",
    "    'turnover_rate': 'turnover' \n",
    "})\n",
    "\n",
    "# multi-index，and transform unstack to wide format\n",
    "df_wide = df_pivot_ready.set_index(['symbol', 'year_month']).unstack('year_month')\n",
    "\n",
    "df_wide.columns = [ (ym, metric) for metric, ym in df_wide.columns ]\n",
    "\n",
    "# rank index\n",
    "df_wide = df_wide.sort_index(axis=1, level=0)\n",
    "\n",
    "df_input_monthly = df_wide\n",
    "\n",
    "#save to the file\n",
    "df_input_monthly.to_csv(\"stock_input.csv\")\n",
    "\n",
    "df_input_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let \"symbol\" in the df be one of the columns in the df (it was the index at the beginning)\n",
    "df_input_monthy = df_input_monthly.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_symbols_with_nan(df, late_date, n):\n",
    "    #obtain the first 2n colume\n",
    "    columns_to_check = df.columns[df.columns.get_loc(late_date) - 2 * n : df.columns.get_loc(late_date)]\n",
    "\n",
    "    # find the column including NaN and return the symbol of the colume\n",
    "    symbols_with_nan = df[df[columns_to_check].isna().any(axis=1)].index\n",
    "\n",
    "    return set(symbols_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_profit_rate_with_symbol(df_input, date_str, date):  # date is the index of the date\n",
    "    \n",
    "    #find the symbol including NaN in the past 40 col\n",
    "    symbols_with_nan = find_symbols_with_nan(df_input, date_str, 40)\n",
    "    \n",
    "    # save the symbol excluded in the NaN set\n",
    "    df_profit_rate = df_input.loc[~df_input.index.isin(symbols_with_nan)]\n",
    "\n",
    "    #the past 40 col + present col (41 col in addition)\n",
    "    selected_columns = list(df_profit_rate.columns[date - 40 : date + 1])\n",
    "\n",
    "    df_profit_rate = df_profit_rate[selected_columns]\n",
    "    df_profit_rate = df_profit_rate.reset_index()\n",
    "\n",
    "    #sort and remove NaN\n",
    "    df_profit_rate = df_profit_rate.sort_values(by=[(date_str, 'profit')], ascending=False).dropna().reset_index(drop=True)\n",
    "\n",
    "    return df_profit_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_profit_rate(df_input, date_str, date):  # date is the index of the date\n",
    "    \n",
    "    #find the symbol including NaN in the past 40 col\n",
    "    symbols_with_nan = find_symbols_with_nan(df_input, date_str, 40)\n",
    "    \n",
    "    # save symbol without NaN\n",
    "    df_profit_rate = df_input.loc[~df_input.index.isin(symbols_with_nan)]\n",
    "    \n",
    "    # choose the 40 col + present 1 col\n",
    "    df_profit_rate = df_profit_rate[df_profit_rate.columns[date - 40:date + 1]]\n",
    "\n",
    "    # sort by (date_str, 'profit')\n",
    "    df_profit_rate = df_profit_rate.sort_values(by=[date_str], ascending=False)\n",
    "\n",
    "    # remove prossible NaN\n",
    "    df_profit_rate = df_profit_rate.dropna().reset_index(drop=True)\n",
    "\n",
    "    return df_profit_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    #calculate the mean and std for each column\n",
    "    mean_values = df.iloc[:, 0:-1].mean()\n",
    "    std_values = df.iloc[:, 0:-1].std()\n",
    "\n",
    "    # normalize except the last colume (for traget label)\n",
    "    for col in df.columns[0:-1]:\n",
    "        df[col] = (df[col] - mean_values[col]) / std_values[col]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first date of the dataset is the 40th col\n",
    "date = 40\n",
    "output_list = []\n",
    "input_list = []\n",
    "#the end of the training dataset is 12/2019, which is the 239th col:\n",
    "while date < 239:\n",
    "\n",
    "    #get the next date\n",
    "    date_str = df_input_monthy.columns[date]\n",
    "\n",
    "    # check whether there are NaN. If included, skip this\n",
    "    symbols_with_nan = find_symbols_with_nan(df_input_monthy, date_str, 20)\n",
    "   \n",
    "    df_profit_rate = create_df_profit_rate(df_input_monthy, date_str, date)\n",
    "\n",
    "    #targte label\n",
    "    #date_str_profit_col = f'{date_str}_profit'\n",
    "        # categrory the stocks based on the t+1 profit with the setting quantile (20%)\n",
    "    quantiles = pd.qcut(df_profit_rate[date_str], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False)\n",
    "    df_profit_rate['quantiles'] = quantiles\n",
    "    \n",
    "        # the quantile to the traget label: [1, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1]\n",
    "    encoding = {0: [0, 0, 1], 1: [0, 0, 0], 2: [0, 1, 0], 3: [0, 0, 0], 4: [1, 0, 0]}\n",
    "    df_profit_rate['encoding'] = df_profit_rate['quantiles'].map(encoding)\n",
    "    \n",
    "        #save these to the list\n",
    "    output_list_temp = df_profit_rate['encoding'].tolist()\n",
    "    output_list = output_list + output_list_temp\n",
    "    \n",
    "    \n",
    "        #z-score normarlize\n",
    "    df_profit_nor = normalize_dataframe(df_profit_rate)\n",
    "    \n",
    "        # get the first 40th col before the target date that everydate contains a vector, combine them\n",
    "    profit_turnoff_cols = [col for col in df_profit_nor.columns if 'profit' in col or 'turnoff' in col]\n",
    "    normalized_df = df_profit_nor.iloc[:, :40]\n",
    "        \n",
    "    vector_list_per_row = normalized_df.apply(lambda row: [list(row[i:i+2]) for i in range(0, len(row), 2)], axis=1)\n",
    "        \n",
    "        #save to the input_daily.txt\n",
    "    input_list_temp = vector_list_per_row.tolist()\n",
    "    input_list = input_list + input_list_temp\n",
    "    \n",
    "    date = date + 2\n",
    "    print(date_str)\n",
    "                  \n",
    "#save this to the output.txt\n",
    "file_path = 'train_output_monthy.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for item in output_list:\n",
    "            file.write(str(item) + '\\n')        \n",
    "        \n",
    "#save to input.txt    \n",
    "# 指定要写入的文本文件路径\n",
    "file_path = 'train_input_monthy.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for item in input_list:\n",
    "            file.write(str(item) + '\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    " \n",
    "    def forward(self, Q, K, V):\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V)\n",
    "\n",
    "        Q = self._split_heads(Q)\n",
    "        K = self._split_heads(K)\n",
    "        V = self._split_heads(V)\n",
    "\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = self._combine_heads(output)\n",
    "\n",
    "        output = self.W_O(output)\n",
    "        return output\n",
    " \n",
    "    def _split_heads(self, tensor):\n",
    "        tensor = tensor.view(tensor.size(0), -1, self.num_heads, self.depth)\n",
    "        return tensor.transpose(1, 2)\n",
    " \n",
    "    def _combine_heads(self, tensor):\n",
    "        tensor = tensor.transpose(1, 2).contiguous()\n",
    "        return tensor.view(tensor.size(0), -1, self.num_heads * self.depth)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    " \n",
    "    def forward(self, x):\n",
    "        attention_output = self.attention(x, x, x)\n",
    "        attention_output = self.norm1(x + attention_output)\n",
    "\n",
    "        feedforward_output = self.feedforward(attention_output)\n",
    "        output = self.norm2(attention_output + feedforward_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.encoder_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    " \n",
    "    def forward(self, x, encoder_output):\n",
    "        self_attention_output = self.self_attention(x, x, x)\n",
    "        self_attention_output = self.norm1(x + self_attention_output)\n",
    "\n",
    "        encoder_attention_output = self.encoder_attention(self_attention_output, encoder_output, encoder_output)\n",
    "        encoder_attention_output = self.norm2(self_attention_output + encoder_attention_output)\n",
    "\n",
    "        feedforward_output = self.feedforward(encoder_attention_output)\n",
    "        output = self.norm3(encoder_attention_output + feedforward_output)\n",
    "        return output\n",
    "\n",
    "class Quantformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, output_dim):\n",
    "        super(Quantformer, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Encoder layers\n",
    "        encoder_output = x.transpose(0, 1)\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output)\n",
    "\n",
    "        # Decoder layers\n",
    "        decoder_output = encoder_output\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output)\n",
    "\n",
    "        # Select the output of the last time step\n",
    "        decoder_output = decoder_output[-1, :, :]\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_layer(decoder_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Quantformer(input_dim=2, hidden_dim=64, num_heads=8, num_layers=6, output_dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file, output_file, num_samples):\n",
    "    with open(input_file, 'r') as f_input, open(output_file, 'r') as f_output:\n",
    "        for _ in range(num_samples):\n",
    "            input_line = f_input.readline().strip()\n",
    "            output_line = f_output.readline().strip()\n",
    "\n",
    "            # Check if either line is empty\n",
    "            if not input_line or not output_line:\n",
    "                continue  # Skip empty lines\n",
    "\n",
    "            try:\n",
    "                input_data = np.array(ast.literal_eval(input_line), dtype=np.float32)\n",
    "                output_data = np.array(ast.literal_eval(output_line), dtype=np.float32)\n",
    "            except SyntaxError:\n",
    "                continue  # Skip lines causing syntax errors\n",
    "\n",
    "            yield input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_samples(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return sum(1 for _ in f)\n",
    "num_samples = count_samples('train_output_monthy_cleaned.txt')\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 166210\n",
    "data_generator = read_data('train_input_monthy_cleaned.txt', 'train_output_monthy_cleaned.txt', num_samples)\n",
    "\n",
    "inputs_monthy_3 = []\n",
    "outputs_monthy_3 = []\n",
    "for _ in range(num_samples):\n",
    "    input_data, output_data = next(data_generator, (None, None))\n",
    "    if input_data is not None and output_data is not None:\n",
    "        inputs_monthy_3.append(input_data)\n",
    "        outputs_monthy_3.append(output_data)\n",
    "\n",
    "inputs_monthy_3 = torch.tensor(inputs_monthy_3, dtype=torch.float32)\n",
    "outputs_monthy_3 = torch.tensor(outputs_monthy_3, dtype=torch.float32)\n",
    "\n",
    "print(inputs_monthy_3.shape)\n",
    "print(outputs_monthy_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset from the inputs and outputs\n",
    "dataset = TensorDataset(inputs_monthy_3, outputs_monthy_3)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 512\n",
    "\n",
    "# Create a DataLoader from the dataset\n",
    "# Note: we set \"shuffle=True\" to shuffle the dataset before each epoch of training\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print the number of batches\n",
    "print(f'Number of batches: {len(data_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Quantformer(input_dim=2, hidden_dim=64, num_heads=8, num_layers=6, output_dim = 3)\n",
    "\n",
    "# Define the loss function (Mean Squared Error Loss)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Initialize the optimizer (Adam optimizer)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Move the model and the loss function to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        # Get the inputs and outputs and move them to the GPU if available\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the outputs by passing inputs to the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass: compute the gradients by performing backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters of the model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss / len(data_loader)))\n",
    "\n",
    "\n",
    "torch.save(model, 'trans_model_monthy_1.pth')\n",
    "torch.save(model.state_dict(), 'trans_params_monthy_1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest base on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_list_at_date(df_input, model_params_path, date, rank, o_dim, frequency):\n",
    "    # determine the date of the column\n",
    "    if frequency == 'monthy':\n",
    "        date_str = df_input.columns[date]  # eg. (\"2020/01\", \"profit\")\n",
    "    else:\n",
    "        column_n = df_input.columns[date]\n",
    "        date_str = column_n.split('_')[0]\n",
    "\n",
    "    # obtain the return df\n",
    "    df_profit_rate = create_df_profit_rate_with_symbol(df_input, date_str, date, frequency)\n",
    "\n",
    "    # Z-score\n",
    "    df_profit_nor = normalize_dataframe(df_profit_rate)\n",
    "\n",
    "    # get profit / turnover col, get the lastest 40 cols\n",
    "    profit_turnoff_cols = [col for col in df_profit_nor.columns if 'profit' in col or 'turnoff' in col]\n",
    "    profit_turnoff_cols = sorted(profit_turnoff_cols)[-40:]  \n",
    "\n",
    "    normalized_df = df_profit_nor[profit_turnoff_cols]\n",
    "\n",
    "    # build the [x,y] vector\n",
    "    vector_list_per_row = normalized_df.apply(\n",
    "        lambda row: [list(row[i:i+2]) for i in range(0, len(row), 2)],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    for symbol, vector in zip(df_profit_rate.index, vector_list_per_row):\n",
    "        data.append([symbol, vector, None])\n",
    "\n",
    "    df_date_rank = pd.DataFrame(data, columns=['symbol', 'vector', 'rank'])\n",
    "    df_date_rank.set_index('symbol', inplace=True) \n",
    "\n",
    "    # get the prediction\n",
    "    df_date_rank = back_test(model_params_path, df_date_rank, rank, o_dim)\n",
    "\n",
    "    return df_date_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_test(model_params_path, test_df, rank, o_dim):\n",
    "    model = Quantformer(input_dim=2, hidden_dim=64, num_heads=8, num_layers=6, output_dim=o_dim)\n",
    "    model.load_state_dict(torch.load(model_params_path))\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    for i in tqdm(range(len(test_df['vector']))):\n",
    "        new_input = test_df['vector'].iloc[i]\n",
    "        new_input_tensor = torch.tensor(new_input, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(new_input_tensor)\n",
    "\n",
    "        output = output.cpu().numpy()\n",
    "        test_df.loc[test_df.index[i], 'rank'] = output[0][0]\n",
    "\n",
    "    # sort by 'rank' \n",
    "    test_df_sorted = test_df.sort_values(by='rank', ascending=False)\n",
    "\n",
    "    # get the col in top N\n",
    "    num_selected = int(len(test_df_sorted) * (rank / 100))\n",
    "    selected_indices = test_df_sorted.index[:num_selected]\n",
    "\n",
    "    selected_df = test_df.loc[selected_indices].copy()\n",
    "\n",
    "    selected_df.reset_index(drop=False, inplace=True) \n",
    "\n",
    "    return selected_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predict stock list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the earliest backtest date 2020/01\n",
    "date = 241 \n",
    "while date < 242: \n",
    "# the latest backtest date 2024/12\n",
    " \n",
    "    date_str = df_input_monthy.columns[date]\n",
    "    desired_date_str = date_str[0]\n",
    "    \n",
    "    #get the predict stocklist at the date\n",
    "    df_temp_daily = predict_list_at_date(df_input_monthy, 'trans_params_monthy.pth', date, 20, 3, 'monthy') \n",
    "    \n",
    "    symbol_values_str = df_temp_daily['symbol'].astype(str).values\n",
    "    desired_length = len(df_monthy_3)\n",
    "    current_length = len(df_temp_daily['symbol'].values)\n",
    "    difference = desired_length - current_length\n",
    "\n",
    "    if difference > 0:\n",
    "        padded_values = np.pad(symbol_values_str, (0, difference), constant_values='nan')\n",
    "        df_monthy_3.loc[:, desired_date_str] = padded_values\n",
    "    else:\n",
    "        df_monthy_3.loc[:, desired_date_str] = symbol_values_str[:desired_length]\n",
    "    \n",
    "    # +2 as there are 2 cols\n",
    "    date = date + 2\n",
    "    print(desired_date_str)\n",
    "    \n",
    "#record in csv\n",
    "df_monthy_3.to_csv('min_20_back_monthy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'trade_monthy.csv'\n",
    "stock_list_data=pd.read_csv(file_path).dropna()\n",
    "\n",
    "def convert_col_to_ym(col):\n",
    "    try:\n",
    "        date = pd.to_datetime(col, format='%Y/%m', errors='coerce')\n",
    "        if pd.notna(date):\n",
    "            return date.strftime('%Y-%m')\n",
    "        else:\n",
    "            return col\n",
    "    except:\n",
    "        return col\n",
    "    \n",
    "# replace the name of the column    \n",
    "stock_list_data.columns = [convert_col_to_ym(col) for col in stock_list_data.columns]\n",
    "stock_list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_in_strategy(start_day, end_day, stock_list, data_list):  \n",
    "    #stock_list is the stock list hold on that month\n",
    "    #data_list the list that save the profit of each month\n",
    "    \n",
    "    #create dataframe\n",
    "    df_profit = {'profit':[]}\n",
    "    date_range = pd.date_range(start = start_day, end = end_day, freq='M')\n",
    "    df_profit = pd.DataFrame(columns=['profit'] + [date.strftime('%Y/%m') for date in date_range])\n",
    "    profit_data = {}\n",
    "    \n",
    "    #change the formation of the time\n",
    "    start_day = pd.to_datetime(start_day, format='%Y%m%d')\n",
    "    end_day = pd.to_datetime(end_day, format='%Y%m%d')\n",
    "    \n",
    "    while start_day != end_day:\n",
    "        profit_temp = 0\n",
    "        column_name = start_day.strftime('%Y/%m')\n",
    "        column_name_1 = start_day.strftime('%Y/%m')\n",
    "        column_name_2 = (start_day.strftime('%Y-%m'), 'profit')\n",
    "        stocklist = stock_list[column_name_1]\n",
    "        for i in range(0,len(stocklist)):\n",
    "            data = data_list.loc[data_list['symbol'] == stocklist[i]] #us stock index are str\n",
    "            profit_stock = data[column_name_2].values[0]\n",
    "            profit_temp = profit_stock + profit_temp\n",
    "        \n",
    "        #calculate profit\n",
    "        profit_temp = profit_temp/len(stocklist)\n",
    "        profit_data[column_name] = profit_temp\n",
    "        #df_profit[column_name][0] = profit_temp\n",
    "        print(start_day.strftime('%Y/%m'))\n",
    "        print(profit_temp)\n",
    "        #to next timestamp\n",
    "        start_day = start_day + pd.DateOffset(months=1, day=1)\n",
    "    \n",
    "    df_profit = pd.DataFrame([profit_data])\n",
    "    return df_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_data = pd.read_csv(\"trade_monthy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the return\n",
    "profit_df = profit_in_strategy('20200101', '20241231', stock_list_data, df_input_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start=\"2020-01-01\", periods=len(returns), freq='M')\n",
    "strategy_returns = pd.Series(returns, index=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = profit_df.iloc[0]\n",
    "\n",
    "nasdaq_returns_percent = [\n",
    "    1.23, -6.78, -11.16, 19.17, 9.31, 6.20, 6.77, 8.54, -5.77, -3.37,\n",
    "    10.79, 4.67, 0.87, -0.26, -1.19, 4.09, -2.02, 4.88, 1.23, 3.39,\n",
    "    -5.62, 6.92, -0.02, -0.68, -9.49, -3.43, 3.41, -13.26, -2.05, -8.71,\n",
    "    12.35, -4.64, -10.50, 3.90, 4.37, -8.73, 10.68, -1.11, 6.69, 0.04,\n",
    "    5.80, 6.59, 4.05, -2.17, -5.81, -2.78, 10.70, 5.52, 1.95, 5.49,\n",
    "    1.67, -4.51, 6.96, 5.14, -0.98, 0.38, 3.43, -0.33, 5.65\n",
    "]\n",
    "# nasdaq return in the past months\n",
    "\n",
    "dates = pd.date_range(start=\"2020-01-01\", periods=len(returns), freq='M')\n",
    "nasdaq_returns = pd.Series(nasdaq_returns_percent, index=dates)\n",
    "\n",
    "\n",
    "returns.index = pd.to_datetime(returns.index, format=\"%Y/%m\")  \n",
    "returns = returns.sort_index() \n",
    "\n",
    "cumulative_returns = (1 + returns/100).cumprod()\n",
    "nasdaq_cumulative = (1 + nasdaq_returns / 100).cumprod()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cumulative_returns, marker='o', label='Quantformer')\n",
    "plt.plot(nasdaq_cumulative, marker='x', label='NASDAQ Composite')\n",
    "plt.title('Cumulative Return Comparison (2020-2024)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "final_cumulative_return = cumulative_returns.iloc[-1]\n",
    "final_nasdaq_return = nasdaq_cumulative.iloc[-1] - 1\n",
    "print(\"Final Cumulative Return:\", final_cumulative_return-1)\n",
    "print(f\"Final Cumulative Return - NASDAQ Composite: {final_nasdaq_return:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the max drawdown (MDD)\n",
    "\n",
    "# cumulative return\n",
    "cumulative_returns = (1 + returns/100).cumprod()\n",
    "\n",
    "# swap to the series\n",
    "cumulative_returns = pd.Series(cumulative_returns.values, index=cumulative_returns.index)\n",
    "\n",
    "# calculate to the highest point\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "\n",
    "# calculate the line of drawdown\n",
    "drawdown = (cumulative_returns - rolling_max) / rolling_max\n",
    "\n",
    "# get the MDD\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "print(f\" Max drawdown: {max_drawdown:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 95% VaR\n",
    "\n",
    "profit_series = profit_df.iloc[0]  \n",
    "\n",
    "# 95% VaR\n",
    "var_95 = profit_series.quantile(0.05)\n",
    "\n",
    "print(f\" 95% VaR: {var_95:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
